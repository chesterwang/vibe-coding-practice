Training or fine-tuning a Sentence Transformers model highly depends on the available data and the target task. The key is twofold:

Understand how to input data into the model and prepare your dataset accordingly.
Know the different loss functions and how they relate to the dataset.
In this tutorial, you will:

Understand how Sentence Transformers models work by creating one from "scratch" or fine-tuning one from the Hugging Face Hub.
Learn the different formats your dataset could have.
Review the different loss functions you can choose based on your dataset format.
Train or fine-tune your model.
Share your model to the Hugging Face Hub.
Learn when Sentence Transformers models may not be the best choice.
How Sentence Transformers models work
In a Sentence Transformer model, you map a variable-length text (or image pixels) to a fixed-size embedding representing that input's meaning. To get started with embeddings, check out our previous tutorial. This post focuses on text.

This is how the Sentence Transformers models work:

Layer 1 â€“ The input text is passed through a pre-trained Transformer model that can be obtained directly from the Hugging Face Hub. This tutorial will use the "distilroberta-base" model. The Transformer outputs are contextualized word embeddings for all input tokens; imagine an embedding for each token of the text.
Layer 2 - The embeddings go through a pooling layer to get a single fixed-length embedding for all the text. For example, mean pooling averages the embeddings generated by the model.
This figure summarizes the process: